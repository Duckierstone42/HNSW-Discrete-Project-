{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Approximate-Nearest-Neighborhood-Search-with-Navigable-Small-World\" data-toc-modified-id=\"Approximate-Nearest-Neighborhood-Search-with-Navigable-Small-World-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Approximate Nearest Neighborhood Search with Navigable Small World</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-Preparation-and-Model\" data-toc-modified-id=\"Data-Preparation-and-Model-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Data Preparation and Model</a></span></li><li><span><a href=\"#Navigable-Small-World\" data-toc-modified-id=\"Navigable-Small-World-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Navigable Small World</a></span></li><li><span><a href=\"#Hnswlib\" data-toc-modified-id=\"Hnswlib-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Hnswlib</a></span></li></ul></li><li><span><a href=\"#Reference\" data-toc-modified-id=\"Reference-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Reference</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../notebook_format'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39m# path : store the current path to convert back to it later\u001b[39;00m\n\u001b[1;32m      5\u001b[0m path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mgetcwd()\n\u001b[0;32m----> 6\u001b[0m os\u001b[39m.\u001b[39;49mchdir(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(\u001b[39m'\u001b[39;49m\u001b[39m..\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m..\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mnotebook_format\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mformats\u001b[39;00m \u001b[39mimport\u001b[39;00m load_style\n\u001b[1;32m      9\u001b[0m load_style(plot_style\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../notebook_format'"
     ]
    }
   ],
   "source": [
    "# code for loading the format for the notebook\n",
    "import os\n",
    "\n",
    "# path : store the current path to convert back to it later\n",
    "path = os.getcwd()\n",
    "os.chdir(os.path.join('..', '..', 'notebook_format'))\n",
    "\n",
    "from formats import load_style\n",
    "load_style(plot_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Author: Ethen\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.11.0\n",
      "IPython version      : 8.16.1\n",
      "\n",
      "numpy   : 1.25.0\n",
      "pandas  : 2.1.2\n",
      "fasttext: 0.9.2\n",
      "scipy   : not installed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "os.chdir(path)\n",
    "\n",
    "# 1. magic for inline plot\n",
    "# 2. magic to print version\n",
    "# 3. magic so that the notebook will reload external python modules\n",
    "# 4. magic to enable retina (high resolution) plots\n",
    "# https://gist.github.com/minrk/3301035\n",
    "%matplotlib inline\n",
    "%load_ext watermark\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "import time\n",
    "import fasttext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# prevent scientific notations\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "%watermark -a 'Ethen' -d -t -v -p numpy,pandas,fasttext,scipy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate Nearest Neighborhood Search with Navigable Small World"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing nearest neighborhood search on embeddings has become a crucial process in many applications, such as similar image/text search. The [ann benchmark](https://github.com/erikbern/ann-benchmarks) contains benchmark on various approximate nearest neighborhood search algorithms/libraries and in this document, we'll take a look at one of them, **Navigable Small World Graph**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation and Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the embedding, we'll be training a fasttext multi-label text classification model ourselves, and using the output embedding for this example. The fasttext library has already been introduced in another post, hence we won't be going over it in detail. The readers can also swap out the data preparation and model section with the embedding of their liking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-10-30 11:32:56--  https://dl.fbaipublicfiles.com/fasttext/data/cooking.stackexchange.tar.gz\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.244.202.103, 18.244.202.62, 18.244.202.73, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|18.244.202.103|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 457609 (447K) [application/x-tar]\n",
      "Saving to: ‘data/cooking.stackexchange.tar.gz’\n",
      "\n",
      "cooking.stackexchan 100%[===================>] 446.88K  1.53MB/s    in 0.3s    \n",
      "\n",
      "2023-10-30 11:32:57 (1.53 MB/s) - ‘data/cooking.stackexchange.tar.gz’ saved [457609/457609]\n",
      "\n",
      "x cooking.stackexchange.id\n",
      "x cooking.stackexchange.txt\n",
      "x readme.txt\n"
     ]
    }
   ],
   "source": [
    "# download the data and un-tar it under the 'data' folder\n",
    "\n",
    "# -P or --directory-prefix specifies which directory to download the data to\n",
    "!wget https://dl.fbaipublicfiles.com/fasttext/data/cooking.stackexchange.tar.gz -P data\n",
    "# -C specifies the target directory to extract an archive to\n",
    "!tar xvzf data/cooking.stackexchange.tar.gz -C data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__sauce __label__cheese How much does potato starch affect a cheese sauce recipe?\n",
      "__label__food-safety __label__acidity Dangerous pathogens capable of growing in acidic environments\n",
      "__label__cast-iron __label__stove How do I cover up the white spots on my cast iron stove?\n"
     ]
    }
   ],
   "source": [
    "!head -n 3 data/cooking.stackexchange.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Tuple\n",
    "\n",
    "def train_test_split_file(input_path: str,\n",
    "                          output_path_train: str,\n",
    "                          output_path_test: str,\n",
    "                          test_size: float=0.1,\n",
    "                          random_state: int=1234,\n",
    "                          encoding: str='utf-8') -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Perform train and test split on a text file without reading the\n",
    "    whole file into memory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_path : str\n",
    "        Path to the original full text file.\n",
    "\n",
    "    output_path_train : str\n",
    "        Path of the train split.\n",
    "\n",
    "    output_path_test : str\n",
    "        Path of the test split.\n",
    "\n",
    "    test_size : float, 0.0 ~ 1.0, default 0.1\n",
    "        Size of the test split.\n",
    "\n",
    "    random_state : int, default 1234\n",
    "        Seed for the random split.\n",
    "\n",
    "    encoding : str, default 'utf-8'\n",
    "        Encoding for reading and writing the file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    count_train, count_test : int\n",
    "        Number of record in the training and test set.\n",
    "    \"\"\"\n",
    "    random.seed(random_state)\n",
    "\n",
    "    # accumulate the number of records in the training and test set\n",
    "    count_train = 0\n",
    "    count_test = 0\n",
    "    train_range = 1 - test_size\n",
    "\n",
    "    with open(input_path, encoding=encoding) as f_in, \\\n",
    "         open(output_path_train, 'w', encoding=encoding) as f_train, \\\n",
    "         open(output_path_test, 'w', encoding=encoding) as f_test:\n",
    "\n",
    "        for line in f_in:\n",
    "            random_num = random.random()\n",
    "            if random_num < train_range:\n",
    "                f_train.write(line)\n",
    "                count_train += 1\n",
    "            else:\n",
    "                f_test.write(line)\n",
    "                count_test += 1\n",
    "\n",
    "    return count_train, count_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def prepend_file_name(path: str, name: str) -> str:\n",
    "    \"\"\"\n",
    "    Prepend the name to the base file name of the input path.\n",
    "    e.g. data/cooking.stackexchange.txt, prepend 'train' to the base file name\n",
    "    data/train_cooking.stackexchange.txt\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Path to a file.\n",
    "\n",
    "    name : str\n",
    "        Name that we'll prepend to the base file name.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    prepended_file_name : str\n",
    "    \"\"\"\n",
    "    directory = os.path.dirname(path)\n",
    "    file_name = os.path.basename(path)\n",
    "    return os.path.join(directory, name + '_' + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train path:  data/train_cooking.stackexchange.txt\n",
      "test path:  data/test_cooking.stackexchange.txt\n"
     ]
    }
   ],
   "source": [
    "# train/test split\n",
    "import os\n",
    "\n",
    "data_dir = 'data'\n",
    "test_size = 0.2\n",
    "input_path = os.path.join(data_dir, 'cooking.stackexchange.txt')\n",
    "input_path_train = prepend_file_name(input_path, 'train')\n",
    "input_path_test = prepend_file_name(input_path, 'test')\n",
    "random_state = 1234\n",
    "encoding = 'utf-8'\n",
    "\n",
    "train_test_split_file(input_path, input_path_train, input_path_test,\n",
    "                      test_size, random_state, encoding)\n",
    "print('train path: ', input_path_train)\n",
    "print('test path: ', input_path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  14496\n",
      "Number of labels: 733\n",
      "Progress:  95.6% words/sec/thread:  119688 lr:  0.004445 avg.loss: 14.606279 ETA:   0h 0m 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  14496\n",
      "label size:  733\n",
      "example vocab:  ['</s>', 'to', 'a', 'How', 'the']\n",
      "example label:  ['__label__baking', '__label__food-safety', '__label__substitutions', '__label__equipment', '__label__bread']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% words/sec/thread:  114568 lr:  0.000000 avg.loss: 14.443398 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "# train the fasttext model\n",
    "fasttext_params = {\n",
    "    'input': input_path_train,\n",
    "    'lr': 0.1,\n",
    "    'lrUpdateRate': 1000,\n",
    "    'thread': 8,\n",
    "    'epoch': 15,\n",
    "    'wordNgrams': 1,\n",
    "    'dim': 80,\n",
    "    'loss': 'ova'\n",
    "}\n",
    "model = fasttext.train_supervised(**fasttext_params)\n",
    "\n",
    "print('vocab size: ', len(model.words))\n",
    "print('label size: ', len(model.labels))\n",
    "print('example vocab: ', model.words[:5])\n",
    "print('example label: ', model.labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output matrix shape:  (733, 80)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 6.342651  , -3.8176107 ,  0.76466626, ..., -3.9752378 ,\n",
       "        -0.95850646, -0.3850138 ],\n",
       "       [ 0.20388015,  1.9417175 , -1.8882055 , ..., -3.995135  ,\n",
       "         0.3488298 , -1.1959981 ],\n",
       "       [ 2.9708028 , -3.3279796 ,  1.1692386 , ..., -0.08886736,\n",
       "         2.0168784 ,  1.0373993 ],\n",
       "       ...,\n",
       "       [ 1.4187317 , -1.147836  ,  0.19228365, ..., -0.70479137,\n",
       "        -0.2099001 ,  0.05079104],\n",
       "       [ 1.4955537 , -1.2106161 ,  0.22310558, ..., -0.60231215,\n",
       "        -0.24376757,  0.03816335],\n",
       "       [ 1.4017599 , -1.1520606 ,  0.2500502 , ..., -0.71590996,\n",
       "        -0.26289156,  0.04611386]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.get_input_matrix().shape\n",
    "print('output matrix shape: ', model.get_output_matrix().shape)\n",
    "model.get_output_matrix()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the output matrix, we would like to compute each of its nearest neighbors using the compressed vectors.\n",
    "\n",
    "For those that are more interested in using some other embeddings, replace the `index_factors` with the embedding, and `query_factors` with a random element from that set of embeddings, and the rest of the document should still function properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__baking\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(80,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we'll get one of the labels to find its nearest neighbors \n",
    "label_id = 0\n",
    "print(model.labels[label_id])\n",
    "\n",
    "index_factors = model.get_output_matrix()\n",
    "query_factors = model.get_output_matrix()[label_id]\n",
    "query_factors.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navigable Small World"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start off by formally defining the problem. k-nearest neighbor search is a problem where given a query object $q$ we need to find the $k$ closest objects from a fixed set of objects $O \\in D$, where $D$ is the set of all possible objects at hand.\n",
    "\n",
    "The idea behind navigable small world is to use a graph data structure $G(V, E)$ to represent these objects $O$, where every object $o_i$ is represented by a vertex/node $v_i$. The navigable small world graph structure is constructed by sequential addition of all elements. For every new element, we find the set of its closest neighbors using a variant of the greedy search algorithm, upon doing so, we'll then introduce a bidirectional connection between that set of neighbors and the incoming element.\n",
    "\n",
    "Upon building the graph, searching for the closest objects to $q$ is very similar to adding objects to the graph. i.e. It involves traversing through the graph to find the closest vertices/nodes using the same variant of greedy search algorithm that's used when constructing the graph.\n",
    "\n",
    "Another thing worth noting is that determining closest neighbors is dependent on a distance function. As the algorithm doesn't make any strong assumption about the data, it can be used on any distance function of our likings. Here we'll be using the cosine distance as an illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    Node for a navigable small world graph.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    idx : int\n",
    "        For uniquely identifying a node.\n",
    "\n",
    "    value : 1d np.ndarray\n",
    "        To access the embedding associated with this node.\n",
    "\n",
    "    neighborhood : set\n",
    "        For storing adjacent nodes.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    https://book.pythontips.com/en/latest/__slots__magic.html\n",
    "    https://hynek.me/articles/hashes-and-equality/\n",
    "    \"\"\"\n",
    "    __slots__ = ['idx', 'value', 'neighborhood']\n",
    "\n",
    "    def __init__(self, idx, value):\n",
    "        self.idx = idx\n",
    "        self.value = value\n",
    "        self.neighborhood = set()\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.idx)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return (\n",
    "            self.__class__ == other.__class__ and\n",
    "            self.idx == other.idx\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "\n",
    "def build_nsw_graph(index_factors, k):\n",
    "    n_nodes = index_factors.shape[0]\n",
    "\n",
    "    graph = []\n",
    "    for i, value in enumerate(index_factors):\n",
    "        node = Node(i, value)\n",
    "        graph.append(node)\n",
    "\n",
    "    for node in graph:\n",
    "        query_factor = node.value.reshape(1, -1)\n",
    "\n",
    "        # note that the following implementation is not the actual procedure that's\n",
    "        # used to find the k closest neighbors, we're just implementing a quick version,\n",
    "        # will come back to this later\n",
    "\n",
    "        # https://codereview.stackexchange.com/questions/55717/efficient-numpy-cosine-distance-calculation\n",
    "        # the smaller the cosine distance the more similar, thus the most\n",
    "        # similar item will be the first element after performing argsort\n",
    "        # since argsort by default sorts in ascending order\n",
    "        dist = distance.cdist(index_factors, query_factor, metric='cosine').ravel()\n",
    "        neighbors_indices = np.argsort(dist)[:k].tolist()\n",
    "        \n",
    "        # insert bi-directional connection\n",
    "        node.neighborhood.update(neighbors_indices)\n",
    "        for i in neighbors_indices:\n",
    "            graph[i].neighborhood.add(node.idx)\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 102, 119, 144, 179, 187, 199, 204, 211, 221}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 10\n",
    "\n",
    "graph = build_nsw_graph(index_factors, k)\n",
    "graph[0].neighborhood"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the original paper, the author used the term \"friends\" of vertices that share an edge, and \"friend list\" of vertex $v_i$ for the list of vertices that share a common with the vertex $v_i$.\n",
    "\n",
    "We'll now introduce the variant of greedy search that the algorithm uses. The pseudocode looks like the following:\n",
    "\n",
    "```\n",
    "greedy_search(q: object, v_entry_point: object):\n",
    "    v_curr = v_entry_point\n",
    "    d_min = dist_func(q, v_current)\n",
    "    v_next = None\n",
    "    \n",
    "    for v_friend in v_curr.get_friends():\n",
    "        d_friend = dist_func(q, v_friend)\n",
    "        if d_friend < d_min:\n",
    "            d_min = d_friend\n",
    "            v_next = v_friend\n",
    "    \n",
    "    if v_next is None:\n",
    "        return v_curr\n",
    "    else:\n",
    "        return greedy_search(q, v_next)\n",
    "```\n",
    "\n",
    "Where starting from some entry point (chosen at random at the beginning), the greedy search algorithm computes a distance from the input query to each of the current entry point's friend vertices. If the distance between the query and the friend vertex is smaller than the current ones, then the greedy search algorithm will move to the vertex and repeats the process until it can't find a friend vertex that is closer to the query than the current vertex.\n",
    "\n",
    "This approach can of course lead to local minimum, i.e. the closest vertex/object determined by this greedy search algorithm is not the actual true closest element to the incoming query. Hence, the idea to extend this is to pick a series of entry point, denoted by `m` in the pseudocode below and return the best results from all those greedy searches. With each additional search, the chances of not finding the true nearest neighbors should decrease exponentially.\n",
    "\n",
    "The key idea behind the knn search is given a random entry point, it iterates on vertices closest to the query that we've never previously visited. And the algorithm keeps greedily exploring the neighborhood until the $k$ nearest elements can't be improved upon. Then this process repeats for the next random entry point.\n",
    "\n",
    "```\n",
    "knn_search(q: object, m: int, k: int):\n",
    "    queue[object] candidates, temp_result, result\n",
    "    set[object] visited_set\n",
    "    \n",
    "    for i in range(m):\n",
    "        put random entry point in candidates\n",
    "        temp_result = None\n",
    "        \n",
    "        repeat:\n",
    "            get element c closet from candidate to q\n",
    "            remove c from candidates\n",
    "            \n",
    "            if c is further than the k-th element from result:\n",
    "                break repeat\n",
    "                \n",
    "            for every element e from friends of c:\n",
    "                if e is not visited_set:\n",
    "                    add e to visited_set, candidates, temp_result\n",
    "                    \n",
    "        \n",
    "        add objects from temp_result to result\n",
    "\n",
    "    return best k elements from result\n",
    "            \n",
    "    \n",
    "```\n",
    "\n",
    "We'll be using the [`heapq`](https://docs.python.org/3/library/heapq.html) module as our priority queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "def nsw_knn_search(\n",
    "    graph: List[Node],\n",
    "    query: np.ndarray,\n",
    "    k: int=5,\n",
    "    m: int=50) -> Tuple[List[Tuple[float, int]], float]:\n",
    "    \"\"\"\n",
    "    Performs knn search using the navigable small world graph.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    graph :\n",
    "        Navigable small world graph from build_nsw_graph.\n",
    "\n",
    "    query : 1d np.ndarray\n",
    "        Query embedding that we wish to find the nearest neighbors.\n",
    "\n",
    "    k : int\n",
    "        Number of nearest neighbors returned.\n",
    "\n",
    "    m : int\n",
    "        The recall set will be chosen from m different entry points.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The list of nearest neighbors (distance, index) tuple.\n",
    "    and the average number of hops that was made during the search.\n",
    "    \"\"\"\n",
    "    result_queue = []\n",
    "    visited_set = set()\n",
    "    \n",
    "    hops = 0\n",
    "    for _ in range(m):\n",
    "        # random entry point from all possible candidates\n",
    "        entry_node = random.randint(0, len(graph) - 1)\n",
    "        entry_dist = distance.cosine(query, graph[entry_node].value)\n",
    "        candidate_queue = []\n",
    "        heapq.heappush(candidate_queue, (entry_dist, entry_node))\n",
    "\n",
    "        temp_result_queue = []\n",
    "        while candidate_queue:\n",
    "            candidate_dist, candidate_idx = heapq.heappop(candidate_queue)\n",
    "\n",
    "            if len(result_queue) >= k:\n",
    "                # if candidate is further than the k-th element from the result,\n",
    "                # then we would break the repeat loop\n",
    "                current_k_dist, current_k_idx = heapq.nsmallest(k, result_queue)[-1]\n",
    "                if candidate_dist > current_k_dist:\n",
    "                    break\n",
    "\n",
    "            for friend_node in graph[candidate_idx].neighborhood:\n",
    "                if friend_node not in visited_set:\n",
    "                    visited_set.add(friend_node)\n",
    "\n",
    "                    friend_dist = distance.cosine(query, graph[friend_node].value)\n",
    "                    heapq.heappush(candidate_queue, (friend_dist, friend_node))\n",
    "                    heapq.heappush(temp_result_queue, (friend_dist, friend_node))\n",
    "                    hops += 1\n",
    "\n",
    "        result_queue = list(heapq.merge(result_queue, temp_result_queue))\n",
    "\n",
    "    return heapq.nsmallest(k, result_queue), hops / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(0, 0),\n",
       "  (0.24626189470291138, 199),\n",
       "  (0.2533913254737854, 221),\n",
       "  (0.3499673008918762, 204),\n",
       "  (0.3559042811393738, 179)],\n",
       " 14.66)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = nsw_knn_search(graph, query_factors, k=5)\n",
    "results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've implemented the knn search algorithm, we can go back and modify the graph building function and use it to implement the actual way of building the navigable small world graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nsw_graph(index_factors: np.ndarray, k: int) -> List[Node]:\n",
    "    n_nodes = index_factors.shape[0]\n",
    "\n",
    "    graph = []\n",
    "    for i, value in enumerate(index_factors):\n",
    "        node = Node(i, value)\n",
    "        if i > k:\n",
    "            neighbors, hops = nsw_knn_search(graph, node.value, k)\n",
    "            neighbors_indices = [node_idx for _, node_idx in neighbors]\n",
    "        else:\n",
    "            neighbors_indices = list(range(i))\n",
    "\n",
    "        # insert bi-directional connection\n",
    "        node.neighborhood.update(neighbors_indices)\n",
    "        for i in neighbors_indices:\n",
    "            graph[i].neighborhood.add(node.idx)\n",
    "        \n",
    "        graph.append(node)\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 21,\n",
       " 23,\n",
       " 24,\n",
       " 27,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 52,\n",
       " 54,\n",
       " 59,\n",
       " 86,\n",
       " 102,\n",
       " 119,\n",
       " 123,\n",
       " 199,\n",
       " 221}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 10\n",
    "\n",
    "index_factors = model.get_output_matrix()\n",
    "graph = build_nsw_graph(index_factors, k)\n",
    "graph[0].neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.342651  , -3.8176107 ,  0.76466626, -0.79287684,  0.03537496,\n",
       "        0.8732404 ,  0.58836055, -0.6819199 , -2.0364614 , -3.843462  ,\n",
       "        0.7229958 , -0.2928096 , -2.221547  , -1.0651853 ,  0.7770709 ,\n",
       "       -0.3025164 ,  0.30859092,  1.1405658 , -0.3937795 , -0.3423791 ,\n",
       "       -1.0514646 ,  1.1292644 ,  0.36772805,  0.5809894 ,  1.987032  ,\n",
       "       -1.987297  ,  0.83476037, -0.91813225, -1.2072208 ,  0.8314127 ,\n",
       "       -0.13114999,  0.09082174, -0.6660503 , -2.4467533 , -1.5038768 ,\n",
       "       -1.7446613 ,  2.535932  , -1.2479595 ,  0.35980675, -3.0065885 ,\n",
       "       -0.32301018, -0.3465733 , -1.454866  , -0.17358883,  0.41430858,\n",
       "        0.32636592,  0.3912431 , -0.57198966, -1.128198  ,  0.3973079 ,\n",
       "       -1.3181994 ,  1.1414636 , -0.69173604, -1.8602835 , -1.5725145 ,\n",
       "       -1.6299673 ,  0.7132161 , -1.0728447 ,  0.20200405,  0.7880225 ,\n",
       "       -0.96887493,  1.8559422 ,  1.7451925 , -0.03650399, -1.1880202 ,\n",
       "       -0.54049027,  0.80976313, -0.29424423, -0.2831327 , -1.2621197 ,\n",
       "        0.5633134 ,  0.9888225 ,  1.0584368 , -1.3120209 , -0.574222  ,\n",
       "       -0.12885708, -1.3211782 , -3.9752378 , -0.95850646, -0.3850138 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_factors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(733, 80)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_factors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.342651  , -3.8176107 ,  0.76466626, -0.79287684,  0.03537496,\n",
       "        0.8732404 ,  0.58836055, -0.6819199 , -2.0364614 , -3.843462  ,\n",
       "        0.7229958 , -0.2928096 , -2.221547  , -1.0651853 ,  0.7770709 ,\n",
       "       -0.3025164 ,  0.30859092,  1.1405658 , -0.3937795 , -0.3423791 ,\n",
       "       -1.0514646 ,  1.1292644 ,  0.36772805,  0.5809894 ,  1.987032  ,\n",
       "       -1.987297  ,  0.83476037, -0.91813225, -1.2072208 ,  0.8314127 ,\n",
       "       -0.13114999,  0.09082174, -0.6660503 , -2.4467533 , -1.5038768 ,\n",
       "       -1.7446613 ,  2.535932  , -1.2479595 ,  0.35980675, -3.0065885 ,\n",
       "       -0.32301018, -0.3465733 , -1.454866  , -0.17358883,  0.41430858,\n",
       "        0.32636592,  0.3912431 , -0.57198966, -1.128198  ,  0.3973079 ,\n",
       "       -1.3181994 ,  1.1414636 , -0.69173604, -1.8602835 , -1.5725145 ,\n",
       "       -1.6299673 ,  0.7132161 , -1.0728447 ,  0.20200405,  0.7880225 ,\n",
       "       -0.96887493,  1.8559422 ,  1.7451925 , -0.03650399, -1.1880202 ,\n",
       "       -0.54049027,  0.80976313, -0.29424423, -0.2831327 , -1.2621197 ,\n",
       "        0.5633134 ,  0.9888225 ,  1.0584368 , -1.3120209 , -0.574222  ,\n",
       "       -0.12885708, -1.3211782 , -3.9752378 , -0.95850646, -0.3850138 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.015845775604248047\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([(0, 0),\n",
       "  (0.24626189470291138, 199),\n",
       "  (0.2533913254737854, 221),\n",
       "  (0.3499673008918762, 204),\n",
       "  (0.3559042811393738, 179)],\n",
       " 14.66)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "results = nsw_knn_search(graph, query_factors, k=5)\n",
    "q_time = time.time() - start\n",
    "print(q_time)\n",
    "results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hnswlib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check our result with a more advanced variant of the algorithm, [**Hierarchical Navigable Small World (HNSW)**](https://arxiv.org/abs/1603.09320) provided by [hnswlib](https://github.com/nmslib/hnswlib). The idea is very similar to skip list data structure, except we now replace its link list with nagivable small world graphs. Although we never formally introduce this new hierarchical variant, but hopefully all its major parameters should look familiar.\n",
    "\n",
    "- `ef`: This algorithm searches for `ef` closest neighbors to the inserted element $q$, this variable was set to $k$ in the original navigable small world paper. These `ef` closest neighbors then becomes the candidate/recall set for inserting bidirectional edges during insertion/construction phase (which is termed `ef_construction`) or after we're done with constructing our graph, these are our candidate/recall set for finding actual top k closest elements to the input query object.\n",
    "- `M`: After choosing `ef_construction` objects, only `M` closest ones will we create edges between the enter point and those nodes. i.e. it controls the number of bi-directional links.\n",
    "\n",
    "The actual process of constructing HNSW and doing knn search is a bit more involved compared to vanilla navigable small world. We won't be getting into all the gory details in this post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hnswlib\n",
    "\n",
    "\n",
    "def build_hnsw(factors, space, ef_construction, M):\n",
    "    # Declaring index\n",
    "    max_elements, dim = factors.shape\n",
    "    hnsw = hnswlib.Index(space, dim) # possible options for space are l2, cosine or ip\n",
    "\n",
    "    # Initing index - the maximum number of elements should be known beforehand\n",
    "    hnsw.init_index(max_elements, M, ef_construction)\n",
    "\n",
    "    # Element insertion (can be called several times)\n",
    "    hnsw.add_items(factors)\n",
    "    return hnsw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.014394998550415039"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space = 'cosine'\n",
    "ef_construction = 200\n",
    "M = 24\n",
    "\n",
    "start = time.time()\n",
    "hnsw = build_hnsw(index_factors, space, ef_construction, M)\n",
    "build_time = time.time() - start\n",
    "build_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0 199 221 204 179]]\n",
      "0.00012302398681640625\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "\n",
    "# Controlling the recall by setting ef, should always be > k\n",
    "hnsw.set_ef(70)\n",
    "\n",
    "# retrieve the top-n search neighbors\n",
    "start = time.time()\n",
    "labels, distances = hnsw.knn_query(query_factors, k=k)\n",
    "query_time = time.time() - start\n",
    "print(labels)\n",
    "print(query_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__label__baking',\n",
       " '__label__baking-powder',\n",
       " '__label__baking-soda',\n",
       " '__label__cupcakes',\n",
       " '__label__cheesecake']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the nearest neighbors and \"translate\" it to the original labels\n",
    "[model.labels[label] for label in labels[0]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the [ann benchmark](https://github.com/erikbern/ann-benchmarks), Hierarchical Navigable Small World (HNSW) stood out as one of the top performing approximate nearest neighborhood algorithms at the time of writing this document. Here, we introduced the vanilla variant of that algorithm, Navigable Small World and also matched the result with a more robust implementation from the open sourced library hnswlib."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Github: Hnswlib - fast approximate nearest neighbor search](https://github.com/nmslib/hnswlib)\n",
    "- [Github: Navigable Small World Graphs For Approximate Nearest Neighbors In Rust](https://github.com/dkohlsdorf/NSWG)\n",
    "- [Paper: Y. Malkov, A. Ponomarenko, A. Logvinov, V. Krylov - Approximate nearest neighbor algorithm based on navigable small world graphs (2014)](https://publications.hse.ru/mirror/pubs/share/folder/x5p6h7thif/direct/128296059)\n",
    "- [Paper: Yu. A. Malkov, D. A. Yashunin - Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs (2016)](https://arxiv.org/abs/1603.09320)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "282.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
